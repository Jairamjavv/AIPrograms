{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76deb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"Loading the Dataset\"\"\"\n",
    "\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509eac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### **Data Preprocessing**\"\"\"\n",
    "\n",
    "words=20000\n",
    "max_length=100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2450aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Padding the Text\"\"\"\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\n",
    "\n",
    "word_size=words\n",
    "word_size\n",
    "\n",
    "embed_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24405da5",
   "metadata": {},
   "source": [
    "### The first argument of the embedding layer is the number of distinct words in the dataset. This argument is defined as large enough so that every word in the corpus can be encoded uniquely. In this project, we have defined the word_size to be 20000. The second argument shows the number of embedding vectors. Each word in the corpus will be shown by the size of the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73d685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          2560000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"### Building a Recurrent Neural Network\"\"\"\n",
    "\n",
    "imdb_model=tf.keras.Sequential()\n",
    "\n",
    "# Embedding Layer\n",
    "imdb_model.add(tf.keras.layers.Embedding(word_size, embed_size, input_shape=(x_train.shape[1],)))\n",
    "\n",
    "# LSTM Layer\n",
    "imdb_model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))\n",
    "\n",
    "# Output Layer\n",
    "imdb_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "imdb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70e0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#### Compiling the model\"\"\"\n",
    "\n",
    "imdb_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 42s 206ms/step - loss: 0.4799 - accuracy: 0.7748\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.2917 - accuracy: 0.8804\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 43s 217ms/step - loss: 0.2304 - accuracy: 0.9099\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 42s 215ms/step - loss: 0.1879 - accuracy: 0.9303\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 42s 216ms/step - loss: 0.1561 - accuracy: 0.9438\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.4775 - accuracy: 0.8323\n",
      "Test accuracy: 0.8322799801826477\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#### Training the model\"\"\"\n",
    "\n",
    "imdb_model.fit(x_train, y_train, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acurracy = imdb_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acurracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f022682",
   "metadata": {},
   "source": [
    "## Source: https://towardsai.net/p/deep-learning/text-classification-with-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be342c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
