{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis of IMDB (Movie Review Dataset) Using LSTM - Postive - Negative - Neutral\n",
    "# available in Keras\n",
    "# also try the same with datasets in UCI or Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "from keras.datasets import imdb\n",
    "vocabulary_size = 5000\n",
    "# train  and test = 2500, 2500 ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and test variables\n",
    "(xtrain, ytrain),(xtest, ytest) = imdb.load_data(num_words = vocabulary_size) # verify : num_words = ramked by how often they occur\n",
    "#try to give num_words = None -> getting an error in model.fit() \n",
    "#num_words: integer or None. Words are ranked by how often they occur (in the training set) and \n",
    "#only the 'num_words' most frequent words are kept. Any less frequent word will appear as oov_char value in the sequence data. \n",
    "#If None, all words are kept. Defaults to None, so all words are kept.\n",
    "#by using num_words, we considering only the top 5000 words in every review sample \n",
    "#every sample in xtrain, ytrain, xtest, ytest will contain only the top 5000 words, other words are OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if data is loaded or not\n",
    "# you will see numerical values as all the words in review dataset are embedded\n",
    "print(xtrain.shape)#there are 25000 movies reviews in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will see the words from dataset say 6th row from dataset\n",
    "print(xtrain[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoid = imdb.get_word_index() #getting the numbers using the function get_word_index()\n",
    "idtoword = {i: word for word, i in wordtoid.items()} #for loop for accessing the words associated with numbers using the function items() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---Reviews as Words---')\n",
    "print([idtoword.get(i,'') for i in xtrain[6]]) # only extracting those words from idtowords that are in the 6th data sample => xtrain[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrain[6]) # what you see here is the embedded form that is, text converted to number form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will see how the output is => Y for thhe 6th sample => positve or Negative - label of data\n",
    "print(ytrain[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to reduce the review length as we have to train the model with same sized input\n",
    "# we will see the max length first\n",
    "#print('Maximum Review Length : {}'.format(len(max(xtrain+xtest),key=len))) # according to the format the values will be assigned to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if theres an unexpected EOF error, look for matching closing parenthesis\n",
    "#len() takes no keyword argument error -> only one arugment inside len()\n",
    "# so the above code is corrected as \n",
    "print('Maximum Review Length : {}'.format(len(max((xtrain + xtest),key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are we checking for maximum length? During training time, all the data should be of same length, so if we know the maximum length in a dataset, we can padd the other samples in dataset accordingly. \n",
    "#or just fix a length and truncate there remaining, to do this, find out the minimuml review length also.\n",
    "print('Maximum Review Length : {}'.format(len(min((xtrain + xtest),key=len))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will do a padding to convert all input data to same length\n",
    "from keras.preprocessing import sequence #padding sequence\n",
    "max_words = 500      #retaining first 500 words and truncating the remaining and padding shorter reviews\n",
    "xtrain = sequence.pad_sequences(xtrain, maxlen = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy paste the same code for Xtest\n",
    "xtest = sequence.pad_sequences(xtest, maxlen = max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LSTM Network\n",
    "from keras import Sequential #to create a NN in a linear/sequential manner\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "embedding_size = 32\n",
    "model= Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words)) # https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation = 'sigmoid')) #last layer in network that will predict if positive (1) or negative (0), so 1 neuron is adequate\n",
    "#print(model.summary) #print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer ='adam', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 #Number of samples trained per iteration\n",
    "num_epochs = 3 #3 #number of times to train the training data, that is, it will subject the training data to training 3 times again and again\n",
    "xvalid, yvalid = xtrain[:batch_size],ytrain[:batch_size] #64 data items from train will be used as xvalidate - its easy for algorithm (learns quickly) if a validation set s involved too\n",
    "xtrain2, ytrain2 = xtrain[batch_size:],ytrain[batch_size:] # remaining is considered as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain2, ytrain2, validation_data =(xvalid, yvalid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(xtest,ytest)\n",
    "print('Test Accuracy : ', scores[1]) #86.167 for 1 epoch, Im getting 83.64 for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
