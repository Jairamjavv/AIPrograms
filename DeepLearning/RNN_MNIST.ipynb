{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c889c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155140ec",
   "metadata": {},
   "source": [
    "## In this model, we're passing the rows of the image as the sequences. So basically, we're showing the the model each pixel row of the image, in order, and having it make the prediction. (28 sequences of 28 elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a530110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09576032",
   "metadata": {},
   "source": [
    "## The type of RNN cell that we're going to use is the LSTM cell. Layers will have dropout, and we'll have a dense layer at the end, before the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9680076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(x_train.shape[1:]), activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb2950",
   "metadata": {},
   "source": [
    "## This should all be straight forward, where rather than Dense or Conv, we're just using LSTM as the layer type. The only new thing is return_sequences. This flag is used for when you're continuing on to another recurrent layer. If you are, then you want to return sequences. If you're not going to another recurrent-type of layer, then you don't set this to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e099ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\InstalledT_S\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 86s 45ms/step - loss: 0.5446 - accuracy: 0.8216 - val_loss: 0.1314 - val_accuracy: 0.9571\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 81s 43ms/step - loss: 0.1505 - accuracy: 0.9581 - val_loss: 0.0874 - val_accuracy: 0.9744\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 88s 47ms/step - loss: 0.1069 - accuracy: 0.9708 - val_loss: 0.0776 - val_accuracy: 0.9766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1384a844a60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea30488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 16ms/step - loss: 0.0776 - accuracy: 0.9766\n",
      "0.9765999913215637\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f27025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
